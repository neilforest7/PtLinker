# 项目设计文档

## 项目概述
本项目旨在构建一个前后端分离的 Web 应用，提供高效的网页爬虫功能，以供**网页登录、签到、数据统计**。前端使用 **React** 和 **Ant Design** 框架构建，后端使用 **SQLite** 数据库存储数据，爬虫功能则使用 **Playwright** 和 **Crawlee** 实现。项目设计为无用户认证和登录，简化配置，便于用户直接使用。

## 技术栈

### 前端
- **语言**: TypeScript
- **框架**: React
- **UI 组件库**: Ant Design
- **状态管理**: Zustand
- **HTTP 请求库**: Axios
- **样式管理**: CSS Modules 或 Styled Components

### 后端
- **框架**: FastAPI
- **数据库**: SQLite + SQLAlchemy 异步模式
- **爬虫框架**: Crawlee 和 Playwright
- **任务队列**: arq (Redis-based)
- **数据验证**: Pydantic v2
- **数据迁移**: Alembic
- **文件存储**: 本地文件系统

## 系统架构设计

### 状态管理设计
使用 Zustand 进行状态管理，主要包含以下 store：

```typescript
// 爬虫任务 Store
interface TaskStore {
  tasks: Task[];
  currentTask: Task | null;
  isLoading: boolean;
  createTask: (config: TaskConfig) => Promise<void>;
  pauseTask: (taskId: string) => Promise<void>;
  resumeTask: (taskId: string) => Promise<void>;
  deleteTask: (taskId: string) => Promise<void>;
}

// 配置 Store
interface ConfigStore {
  configs: CrawlerConfig[];
  templates: ConfigTemplate[];
  currentConfig: CrawlerConfig | null;
  saveConfig: (config: CrawlerConfig) => Promise<void>;
  validateConfig: (config: CrawlerConfig) => Promise<ValidationResult>;
}

// 数据 Store
interface DataStore {
  crawledData: CrawledData[];
  pagination: PaginationConfig;
  filters: FilterConfig;
  fetchData: (params: QueryParams) => Promise<void>;
  exportData: (format: ExportFormat) => Promise<void>;
}
```

### API 接口设计

```typescript
// 任务管理接口
interface TaskAPI {
  '/api/tasks': {
    GET: () => Task[]
    POST: (config: TaskConfig) => Task
  }
  '/api/tasks/:taskId': {
    GET: () => Task
    PUT: (updates: TaskUpdate) => Task
    DELETE: () => void
  }
  '/api/tasks/:taskId/status': {
    PUT: (status: TaskStatus) => void
  }
  '/api/tasks/batch': {
    POST: (tasks: TaskConfig[]) => Task[]
  }
}

// 配置管理接口
interface ConfigAPI {
  '/api/configs': {
    GET: () => CrawlerConfig[]
    POST: (config: CrawlerConfig) => CrawlerConfig
  }
  '/api/templates': {
    GET: () => ConfigTemplate[]
  }
  '/api/configs/validate': {
    POST: (config: CrawlerConfig) => ValidationResult
  }
  '/api/configs/test': {
    POST: (config: CrawlerConfig) => TestResult
  }
}

// 数据管理接口
interface DataAPI {
  '/api/data': {
    GET: (params: QueryParams) => PagedData
  }
  '/api/data/export': {
    POST: (format: ExportFormat) => {
      taskId: string;  // 返回后台任务ID
      status: 'processing'
    }
  }
  '/api/data/export/:taskId': {
    GET: () => {
      status: 'processing' | 'completed' | 'failed';
      downloadUrl?: string;
    }
  }
}
```

### 实时更新机制
使用 FastAPI 的原生 WebSocket 支持实现实时通信：

```typescript
interface WebSocketEvents {
  // 任务状态更新
  'task:status': {
    taskId: string;
    status: TaskStatus;
    progress?: number;
    error?: string;
    memory_usage?: number;
    cpu_usage?: number;
  }
  
  // 日志更新
  'task:log': {
    taskId: string;
    log: LogEntry;
    level: 'info' | 'warning' | 'error';
    timestamp: string;
  }
  
  // 数据更新
  'data:update': {
    taskId: string;
    newData: CrawledData;
    batch_size?: number;
  }
}
```

## 项目结构

### 前端项目结构
```
frontend/
│
├── public/                   # 公共静态文件
│
├── src/
│   ├── components/          
│   │   ├── common/         # 通用组件
│   │   ├── crawler/        # 爬虫相关组件
│   │   ├── task/          # 任务管理组件
│   │   └── data/          # 数据展示组件
│   ├── hooks/             
│   │   ├── useWebSocket.ts # WebSocket 钩子
│   │   ├── useTask.ts     # 任务管理钩子
│   │   └── useConfig.ts   # 配置管理钩子
│   ├── stores/            
│   │   ├── taskStore.ts   # 任务状态管理
│   │   ├── configStore.ts # 配置状态管理
│   │   └── dataStore.ts   # 数据状态管理
│   ├── types/             
│   ├── utils/             
│   └── services/          
│   └── App.js              # 主应用入口
│
└── package.json              # 项目依赖和脚本
```

### 后端项目结构
```
backend/
│
├── app/
│   ├── main.py               # FastAPI 应用入口
│   ├── core/                 # 核心功能模块
│   │   ├── config.py        # 配置管理
│   │   ├── logging.py       # 日志配置
│   │   └── events.py        # 事件管理
│   ├── models/              # 数据模型
│   │   ├── pydantic/        # Pydantic 模型
│   │   └── sqlalchemy/      # SQLAlchemy 模型
│   ├── api/                 # API 路由
│   │   ├── v1/             # API 版本
│   │   └── websockets.py   # WebSocket 处理
│   ├── services/           # 业务逻辑
│   │   ├── crawler.py      # 爬虫服务
│   │   └── data.py         # 数据处理服务
│   ├── workers/            # 异步任务处理
│   │   ├── crawler.py      # 爬虫任务
│   │   └── export.py       # 导出任务
│   ├── db/                 # 数据库
│   │   ├── session.py      # 数据库会话
│   │   └── migrations/     # Alembic 迁移
│   └── utils/              # 工具函数
│
├── tests/                  # 测试用例
│   ├── unit/              
│   └── integration/       
│
└── requirements.txt        # Python 依赖
```

## 功能需求

1. **数据爬取**: 利用 Crawlee 和 Playwright 进行网页数据爬取，爬取结果存储在 SQLite 数据库中。
2. **数据展示**: 使用 React 和 Ant Design 创建用户界面，展示爬取的数据。
3. **无用户认证**: 项目无需用户登录，用户只需按照说明配置后即可使用，不需要额外转发请求。
4. **配置简单**: 提供详细的配置指南，用户只需填写少量参数即可运行爬虫。
5. **爬虫配置界面**
- 实现可视化的爬虫规则配置
- 提供预设模板
- 实时验证配置有效性
6. **任务管理**
- 爬虫任务的创建、暂停、继续、删除
- 任务执行状态实时展示
- 任务执行日志实时展示
7. **数据展示**
- 支持多种数据展示方式（表格、图表等）
- 提供数据筛选和搜索功能
- 支持数据导出


## 爬虫逻辑

### 使用 Crawlee 和 Playwright 
- **Crawlee**: 作为核心爬虫库，配置请求头、代理等。
- **Playwright**: 处理动态页面和复杂的用户交互。

代码示例（`crawlers/crawler.py`）：
```python
from crawlee import PlaywrightCrawler

async def handle_request(request):
    # 处理请求和数据存储逻辑
    pass

crawler = PlaywrightCrawler(
    handle_request=handle_request,
    headless=True
)

if __name__ == "__main__":
    crawler.start('https://example.com')
```

## 部署

- **环境依赖**:
  
  在 `requirements.txt` 和 `package.json` 中列出所有必需的库和依赖。

- **数据库初始化**:
  
  在初始化阶段，自动生成 SQLite 数据库和必要的数据表。

- **启动项目**:
  
  前端:
  ```bash
  cd frontend
  npm install
  npm start
  ```

  后端:
  ```bash
  cd backend
  pip install -r requirements.txt
  uvicorn app.main:app --reload
  ```

## 文档和支持

- **README.md**: 提供完整的项目文档，包括功能说明、安装步骤、配置指导和使用说明。
- **问题反馈**: 提供沟通渠道（如 GitHub Issues），以便用户反馈问题。

## 错误处理机制

1. **前端错误处理**
- API 请求错误统一处理
- 友好的错误提示界面
- 任务异常状态处理

2. **后端错误处理**
- 爬虫异常重试机制
- 数据库事务管理
- 日志记录系统

## 性能优化策略

1. **前端优化**
- 组件懒加载
- 虚拟列表处理大数据
- WebSocket 断线重连

2. **后端优化**
- 数据分页查询
- 缓存机制
- 异步任务处理

## 可维护性设计

1. **模块化爬虫系统**
- 插件式规则配置
- 中间件扩展机制
- 自定义处理器支持

2. **监控和调试**
- 详细的日志记录
- 性能指标收集
- 调试模式支持

3. **测试策略**
- 单元测试覆盖
- 集成测试
- 爬虫模拟测试

## 总结
该项目采用现代化的异步架构设计，通过任务队列系统处理长时间运行的爬虫任务，使用 WebSocket 实现实时数据更新，并实现了完善的错误处理和性能优化机制。项目的模块化设计和完整的测试覆盖确保了代码的可维护性和可靠性。